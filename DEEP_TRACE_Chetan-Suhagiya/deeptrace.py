# -*- coding: utf-8 -*-
"""DeepTrace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fy8ex3EE1Zsu7-3SZEzydENM5H9yYj16
"""

import requests
import re
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import concurrent.futures
import sys

# --- Configuration ---

USE_AI = True
CONFIDENCE_THRESHOLD = 0.8  # Threshold to flag AI detection to reduce false positives

# Regex patterns for secret detection -- expand as needed
regex_patterns = {
    "AWS Access Key": r"AKIA[0-9A-Z]{16}",
    "Google API Key": r"AIza[0-9A-Za-z-_]{35}",
    "Generic Password": r"(?i)password\s*=\s*.*",
    "JWT Secret": r"(?i)jwt[_\-]?secret\s*=\s*.*",
    "Stripe Secret Key": r"sk_live_[0-9a-zA-Z]{24,}",
    "Stripe Test Key": r"sk_test_[0-9a-zA-Z]{24,}",
    "Generic API Key": r"(?i)api[_\-]?key\s*=\s*.*",
    "Bearer Token": r"Bearer\s+[A-Za-z0-9\-\._~\+\/]+=*",
    "Database URL": r"(?i)database[_\-]?url\s*=\s*.*",
    "Private Key Begin": r"-----BEGIN (RSA|DSA|EC|OPENSSH|PGP) PRIVATE KEY-----",
    "SSH Private Key Begin": r"-----BEGIN OPENSSH PRIVATE KEY-----",
    "Azure Storage Key": r"(?i)azure.*key\s*=\s*[A-Za-z0-9+/=]{40,}",
    # Add more patterns as needed...
}

# --- Load AI Model & Tokenizer ---

print("Loading AI model and tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base", num_labels=2)
model.eval()
if torch.cuda.is_available():
    model.to("cuda")

# --- Detection Functions ---

def is_sensitive_ai(line):
    """Use AI model with confidence threshold to detect sensitive lines."""
    inputs = tokenizer(line, return_tensors="pt", truncation=True, max_length=512)
    if torch.cuda.is_available():
        inputs = {k:v.to("cuda") for k,v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    conf_score, pred_label = torch.max(probs, dim=1)
    return (pred_label.item() == 1) and (conf_score.item() >= CONFIDENCE_THRESHOLD)

def is_sensitive_regex(line):
    """Check if line matches any regex pattern."""
    for key, pattern in regex_patterns.items():
        if re.search(pattern, line):
            return key
    return None

# --- File Scanning ---

def scan_file_contents(lines, file_name, scan_results):
    """Scan each line of file contents for secrets."""
    found_any = False
    for i, line in enumerate(lines):
        line_strip = line.strip()
        if not line_strip:
            continue
        regex_result = is_sensitive_regex(line_strip)
        if regex_result:
            print(f"Error [Regex] {regex_result} in {file_name}, line {i+1}: {line_strip}")
            scan_results["issues_found"] += 1
            found_any = True
        elif USE_AI:
            try:
                if is_sensitive_ai(line_strip):
                    print(f"Error [AI] Sensitive line in {file_name}, line {i+1}: {line_strip}")
                    scan_results["issues_found"] += 1
                    found_any = True
            except Exception as e:
                print(f"Warning: AI detection failed on line {i+1} in {file_name}: {e}", file=sys.stderr)
    if not found_any:
        print(f"No sensitive lines detected in {file_name}.")
        scan_results["secure_files"] += 1
    scan_results["files_scanned"] += 1


def scan_file_from_url(file_url, file_name, scan_results):
    """Fetch and scan file contents from raw URL."""
    print(f"\nScanning file: {file_name} ...")
    try:
        response = requests.get(file_url, timeout=10)
        if response.status_code == 404:
            print(f"File not found (404): {file_url}")
            return
        if response.status_code != 200:
            print(f"Failed to fetch file {file_url}: Status {response.status_code}")
            return
    except requests.RequestException as e:
        print(f"Network error fetching {file_url}: {e}")
        return

    # Filtering out very large files (optional, max ~1MB)
    if len(response.content) > 1_000_000:
        print(f"Skipping very large file {file_name} (>1 MB)")
        return

    content = response.text
    lines = content.splitlines()
    scan_file_contents(lines, file_name, scan_results)


# --- Recursive repo scanning ---

def scan_github_repo(owner, repo, path="", scan_results=None):
    """Recursively scan all files in GitHub repo via contents API."""
    if scan_results is None:
        scan_results = {"files_scanned":0, "issues_found":0, "secure_files":0}

    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"
    headers = {"Accept": "application/vnd.github.v3+json"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 404:
            print(f"Path not found: {url}")
            return scan_results
        if response.status_code != 200:
            print(f"Error accessing repo path {path}: HTTP {response.status_code}")
            return scan_results
        contents = response.json()
    except requests.RequestException as e:
        print(f"Network error accessing {url}: {e}")
        return scan_results

    # Use ThreadPoolExecutor to parallelize file scans for speed
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
        futures = []
        for item in contents:
            if item['type'] == 'file':
                file_url = item['download_url']
                file_name = path + item['name']
                futures.append(executor.submit(scan_file_from_url, file_url, file_name, scan_results))
            elif item['type'] == 'dir':
                # Recursive call for directory
                scan_github_repo(owner, repo, path=path + item['name'] + '/', scan_results=scan_results)

        # Wait for all file scan futures to complete
        concurrent.futures.wait(futures)

    return scan_results


# --- CLI Interface ---

def main():
    print("=== DeepTrace-AI: AI-Powered Secret Leak Detector ===\n")
    print("Select an option:")
    print("1) Scan default Laravel .env.example file")
    print("2) Scan GitHub repository (recursively)")
    print("3) Scan raw file URL")
    print("q) Quit")

    choice = input("Enter choice (1/2/3/q): ").strip().lower()

    scan_results = {"files_scanned": 0, "issues_found": 0, "secure_files": 0}

    if choice == '1':
        laravel_env_url = "https://raw.githubusercontent.com/laravel/laravel/master/.env.example"
        file_name = ".env.example"
        scan_file_from_url(laravel_env_url, file_name, scan_results)

    elif choice == '2':
        owner = input("Enter GitHub owner (e.g., laravel): ").strip()
        repo = input("Enter repository name (e.g., laravel): ").strip()
        print(f"\nStarting scan of GitHub repo: {owner}/{repo}")
        scan_results = scan_github_repo(owner, repo, scan_results=scan_results)

    elif choice == '3':
        file_url = input("Enter raw file URL: ").strip()
        file_name = file_url.split("/")[-1]
        scan_file_from_url(file_url, file_name, scan_results)

    elif choice == 'q':
        print("Exiting.")
        return

    else:
        print("Invalid choice.")
        return

    # Summary report
    print("\n=== Scan Summary ===")
    print(f"Total files scanned: {scan_results['files_scanned']}")
    print(f"Files with sensitive info detected: {scan_results['files_scanned'] - scan_results['secure_files']}")
    print(f"Total issues found: {scan_results['issues_found']}")
    print(f"Secure files: {scan_results['secure_files']}")
    print("=== Scan complete ===\n")


if __name__ == "__main__":
    main()